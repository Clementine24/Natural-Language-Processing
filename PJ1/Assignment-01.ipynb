{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0\n",
    "Load the preprocessed data records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import math\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "wiki_data = []\n",
    "with open(\"enwiki_20220201.json\",\"r\") as f:\n",
    "    for each_line in f:\n",
    "        record = json.loads(each_line)\n",
    "        wiki_data.append(record)\n",
    "print(len(wiki_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Data exploring and preprocessing.  \n",
    "Use some simple methods to explore the data and obtain some information about the nature of the data, so as to facilitate the later language processing.  \n",
    "First, let's take a look at the basic structure of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'label', 'text'])\n"
     ]
    }
   ],
   "source": [
    "print(wiki_data[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that each text data consists of **title**, **label** and **text**. Let's further observe these three elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the first data: \n",
      " Citizen_Kane\n",
      "The label of the first data: \n",
      " Film\n",
      "The text of the first data: \n",
      " Citizen Kane is a 1941 American drama film produced by, directed by, and starring Orson Welles. He also co-wrote the screenplay with Herman J. Mankiewicz. The picture was Welles' first feature film.  \n",
      "Because the text is too long, only a small part is shown here.\n"
     ]
    }
   ],
   "source": [
    "print('The title of the first data: \\n', wiki_data[0]['title'])\n",
    "print('The label of the first data: \\n', wiki_data[0]['label'])\n",
    "print('The text of the first data: \\n', wiki_data[0]['text'][:199],'\\nBecause the text is too long, only a small part is shown here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Print out how many documents are in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(data):\n",
    "    class_counter = {}\n",
    "    sent_counter = {}\n",
    "    token_counter = {}\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')  # 设定一个分词器\n",
    "    processed_data = []\n",
    "    for text_dic in tqdm(data):\n",
    "        processed_text = []\n",
    "        text_dic['processed_data'] = []\n",
    "        # 统计文档的类别，并初始化没有统计过的类别\n",
    "        label = text_dic['label']\n",
    "        if label not in class_counter:\n",
    "            class_counter[label] = 1\n",
    "            sent_counter[label] = 0\n",
    "            token_counter[label] = 0\n",
    "        else:\n",
    "            class_counter[label] += 1\n",
    "        for sent in nltk.sent_tokenize(text_dic['text']):\n",
    "            # 统计每个类别的句子数\n",
    "            sent_counter[label] += 1\n",
    "            tokens = nltk.word_tokenize(sent)\n",
    "            # 统计每个类别的token数\n",
    "            token_counter[label] += len(tokens)\n",
    "            # 处理每条句子，使之只保留英文单词和数字，且所有英文单词小写\n",
    "            processed_text.append(tokenizer.tokenize(sent.lower()))\n",
    "        processed_data.append(processed_text)\n",
    "    for key in class_counter.keys():\n",
    "        sent_counter[key] = round(sent_counter[key]/class_counter[key],2)\n",
    "        token_counter[key] = round(token_counter[key]/class_counter[key],2)\n",
    "    return class_counter, sent_counter, token_counter, processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [14:45<00:00, 11.30it/s]\n"
     ]
    }
   ],
   "source": [
    "class_counter, sent_counter, token_counter, processed_data = counter(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Film': 3048,\n",
       " 'Book': 975,\n",
       " 'Politician': 3824,\n",
       " 'Writer': 837,\n",
       " 'Food': 137,\n",
       " 'Actor': 80,\n",
       " 'Animal': 93,\n",
       " 'Software': 266,\n",
       " 'Artist': 520,\n",
       " 'Disease': 220}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印出每个类有多少个文档\n",
    "class_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Film': 178.62,\n",
       " 'Book': 205.26,\n",
       " 'Politician': 225.29,\n",
       " 'Writer': 217.89,\n",
       " 'Food': 155.43,\n",
       " 'Actor': 70.95,\n",
       " 'Animal': 66.81,\n",
       " 'Software': 202.62,\n",
       " 'Artist': 185.04,\n",
       " 'Disease': 349.6}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印出每个类的平均句子数\n",
    "sent_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Film': 4440.56,\n",
       " 'Book': 5296.75,\n",
       " 'Politician': 5708.72,\n",
       " 'Writer': 5806.98,\n",
       " 'Food': 3477.58,\n",
       " 'Actor': 1719.95,\n",
       " 'Animal': 1432.14,\n",
       " 'Software': 4813.02,\n",
       " 'Artist': 4802.04,\n",
       " 'Disease': 8012.64}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印出每个类的平均token数\n",
    "token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除变量，节省内存\n",
    "del class_counter\n",
    "del sent_counter\n",
    "del token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['citizen',\n",
       " 'kane',\n",
       " 'is',\n",
       " 'a',\n",
       " '1941',\n",
       " 'american',\n",
       " 'drama',\n",
       " 'film',\n",
       " 'produced',\n",
       " 'by']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印出处理后的文本，检查一下\n",
    "processed_data[0][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本中确实只剩英文单词和数字，且所有英文字母小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除变量，节省内存\n",
    "del processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储处理后的数据，以后就不用再处理了,因为数据量太大了存不下，分为两个文件保存\n",
    "# path = 'processed_data1'\n",
    "# with open(path, 'wb') as f:\n",
    "#     pickle.dump(processed_data[:5000], f)\n",
    "# f.close()\n",
    "# path = 'processed_data2'\n",
    "# with open(path, 'wb') as f:\n",
    "#     pickle.dump(processed_data[5000:], f)\n",
    "# f.close()\n",
    "# 加载数据\n",
    "path = 'processed_data1'\n",
    "f1 = open(path, 'rb')\n",
    "data = pickle.load(f1)\n",
    "data[0][0][:10]\n",
    "path = 'processed_data2'\n",
    "f1 = open(path, 'rb')\n",
    "data.extend(pickle.load(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先要做的是对数据集进行划分，主要分为两部分：  \n",
    "1. 打乱数据集，避免其出现聚集现象导致模型效果降低\n",
    "2. 划分数据集，使用9:1的比例划分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, seed, train_porp):\n",
    "    '''\n",
    "    输入待划分数据，随机种子以及训练集比例\n",
    "    根据题目，我们暂且将训练集的比例固定为10%(最后的)\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    # 打乱数据集\n",
    "    np.random.shuffle(data)\n",
    "    # 划分数据集\n",
    "    num_train = math.floor(len(data)*train_porp)\n",
    "    num_test = math.floor(len(data)*0.9)\n",
    "    return data[:num_train], data[num_test:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 1000\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "train_data, test_data = split_data(data,1,0.9)\n",
    "print(len(train_data),len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先要对数据集进行进一步的处理，主要包括两部分：  \n",
    "1. 对每一句话的首尾分别加上<s>,</s>表示语句的开始和结束，另外一个作用是保证一个句子里的每一个单词在分词后的词组中出现的次数是一致的。要注意如果ngram的n大于2的话，要加上n-1个首尾标识符。\n",
    "2. 对所有的token进行初步的统计，如果出现的频数过小，比如一次，可以直接将其设置为<UNK>,这样做最大的好处是对数据进行了降维，另外一个也可以防止过拟合，如果只出现过一次，我们可以认为它是一个非常稀少的词，不考虑再次出现的情况，有利于增强模型的泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_s(data, n):\n",
    "    processed_text = []\n",
    "    start = ['<s>']*max(1,n-1)\n",
    "    end = '</s>'  # 这里只拼一个是因为后面要把所有句子合并在一起再划分窗口，如果end的数量超过一个，会出现好几种切割出来的元组只有start和end的情况\n",
    "    for text in tqdm(data):\n",
    "        processed_text.extend(start)\n",
    "        processed_text.extend(text)\n",
    "        processed_text.append(end)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unique(text):\n",
    "    vocab = nltk.FreqDist(text)\n",
    "    return [token if vocab[token] > 1 else '<UNK>' for token in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predata(text_data, n):\n",
    "    '''\n",
    "    Input：\n",
    "    text_data:一个列表，里面每个元素为预处理的分词之后句子列表\n",
    "    n:ngram的词元数\n",
    "    '''\n",
    "    processed_text = add_s(text_data, n)\n",
    "    return drop_unique(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1840809/1840809 [00:02<00:00, 843932.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# 先将列表展开一层\n",
    "flatten_data = [y for x in train_data for y in x]\n",
    "pretrain_3 = predata(flatten_data,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207840/207840 [00:00<00:00, 577857.03it/s]\n"
     ]
    }
   ],
   "source": [
    "flatten_data = [y for x in test_data for y in x]\n",
    "pretest_3 = predata(flatten_data,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存已经预处理好的数据\n",
    "# path = 'trigram_train1'\n",
    "# with open(path, 'wb') as f:\n",
    "#     pickle.dump(pretrain_3, f)\n",
    "# f.close()\n",
    "path = 'trigram_test'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(pretest_3, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "手动实现laplace平滑和KN平滑的Ngram模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM():\n",
    "    '''\n",
    "    定义语言模型这个类，可以实现ngram及laplace、kneser-Ney平滑方法可选\n",
    "    '''\n",
    "    def __init__(self, train_data, n, laplace=False, KN = False):\n",
    "        '''\n",
    "        Input:\n",
    "        train_data：处理过后的训练集，为一个列表，每个元素为一个token\n",
    "        n：ngram的窗口大小\n",
    "        laplace：取值为False，代表不使用这个平滑；否则其取值为k,代表实现加k平滑\n",
    "        KN：取值为False，代表不使用这个平滑；否则其取值为d,代表折扣系数\n",
    "        '''\n",
    "        self.token = train_data\n",
    "        self.n = n\n",
    "        self.vocab = nltk.FreqDist(train_data)\n",
    "        self.vocabsize = len(self.vocab)\n",
    "        # 注意这里的话，如果是laplace模型，那么就只生成了ngram的数据集，但是如果是KNModel，可以生成所有小于等于n的数据集  \n",
    "        self.masks = list(reversed(list(product((0,1), repeat=n))))  # 后面为了匹配测试集与训练集的元组需要的，取反是为了实现最大匹配原则\n",
    "        self.laplace = laplace\n",
    "        self.KN = KN\n",
    "        self.model = self.laplaceModel() if laplace else self.KNModel()\n",
    "        \n",
    "    def LpSmooth(self):\n",
    "        '''\n",
    "        实现n>1的laplace平滑\n",
    "        '''\n",
    "        # 统计ngram的词典\n",
    "        ngram = nltk.ngrams(self.token, self.n)\n",
    "        nVocab = nltk.FreqDist(ngram)\n",
    "        # 统计ngram前面的N-1的词典\n",
    "        pre_gram = nltk.ngrams(self.token, self.n-1)\n",
    "        pre_Vocab = nltk.FreqDist(pre_gram)\n",
    "        # 计算laplace平滑后的概率\n",
    "        def Lpprob(gram, count):\n",
    "            pre = gram[:-1]\n",
    "            pre_count = pre_Vocab[pre]\n",
    "            return (count + self.laplace)/(pre_count+self.laplace*self.vocabsize)\n",
    "        Lpdict = {gram:Lpprob(gram, count) for gram, count in nVocab.items()}\n",
    "        return Lpdict\n",
    "        \n",
    "    def laplaceModel(self):\n",
    "        '''\n",
    "        laplace平滑模型\n",
    "        '''\n",
    "        # 先考虑unigram的特殊情况\n",
    "        if self.n == 1:\n",
    "            den = len(self.token) # 分母\n",
    "            Lpdict = {(gram,): count/den for gram, count in self.vocab.items()} # 为了保证输出时的key是tuple\n",
    "            return Lpdict\n",
    "        return self.LpSmooth()\n",
    "    \n",
    "    def KNModel(self):\n",
    "        '''\n",
    "        KN平滑模型\n",
    "        '''\n",
    "        kgram_counts = [nltk.FreqDist(nltk.ngrams(self.token, self.n))]\n",
    "        for i in range(1, self.n-1):\n",
    "            kgram_counts.append(nltk.FreqDist(nltk.ngrams(self.token, self.n-i)))\n",
    "        den = len(self.token)\n",
    "        kgram_counts.append({(gram,): count/den for gram, count in self.vocab.items()})\n",
    "        probs = self.cal_probs(kgram_counts)\n",
    "        return probs  # 这里返回的概率是所有小于等于n的KN平滑后概率，因此n=1,2,3只用做一次\n",
    "    \n",
    "    def cal_probs(self, orders):\n",
    "        '''\n",
    "        计算所有等级（n）的平滑概率\n",
    "        '''\n",
    "        backoffs = []\n",
    "        for order in orders[:-1]:\n",
    "            backoff = self.cal_order_backoff_probs(order)\n",
    "            backoffs.append(backoff)\n",
    "#         orders[-1] = self.cal_unigram_probs(order[-1])  # 当n等于1时要单独计算\n",
    "        backoffs.append(defaultdict(int))  # n等于1时的backoff为0\n",
    "        self.interpolate(orders, backoffs)  # 将上面的概率插值求出来\n",
    "        return orders\n",
    "    \n",
    "#     def cal_unigram_probs(self, unigrams):\n",
    "#         den = sum(value for value in unigrams.values())  # 计算分母\n",
    "#         unigrams = {key: math.log(value/den) for key, value in unigrams.items()}\n",
    "#         return unigrams\n",
    "    \n",
    "    def cal_order_backoff_probs(self, order):\n",
    "        '''\n",
    "        分别计算KN平滑的前后两部分\n",
    "        '''\n",
    "        prefix_sums = defaultdict(int)\n",
    "        backoffs = defaultdict(int)\n",
    "        for key in order.keys():\n",
    "            prefix = key[:-1]\n",
    "            count = order[key]\n",
    "            prefix_sums[prefix] += count\n",
    "            order[key] -= self.KN  # 减去折扣值\n",
    "            backoffs[prefix] += self.KN\n",
    "        for key in order.keys():\n",
    "            prefix = key[:-1]\n",
    "            order[key] = order[key]/prefix_sums[prefix]  #计算discounted ngram\n",
    "        for prefix in backoffs.keys():\n",
    "            backoffs[prefix] = backoffs[prefix]/prefix_sums[prefix]\n",
    "        return backoffs  # 其实是返回order和backoff两部分，但由于order是直接输入的地址，因此无需传出\n",
    "    \n",
    "    def interpolate(self, orders, backoffs):\n",
    "        '''\n",
    "        计算所有KN平滑过后的概率\n",
    "        '''\n",
    "        for pre_order, order, backoff in zip(reversed(orders),reversed(orders[:-1]),\n",
    "                                            reversed(backoffs[:-1])):\n",
    "            for kgram in order.keys():\n",
    "                prefix, suffix = kgram[:-1], kgram[1:]\n",
    "                order[kgram] += pre_order[suffix]*backoff[prefix] # 动态更新ngram的KN平滑概率\n",
    "    \n",
    "    def perplexity(self, test_data, n_of_KN = None):\n",
    "        '''\n",
    "        用已经生成的模型计算给定测试数据的困惑度，要求test_data也是处理过的\n",
    "        '''\n",
    "        test_gram = nltk.ngrams(test_data, self.n) if self.laplace else nltk.ngrams(test_data, n_of_KN)\n",
    "        N = len(test_data)\n",
    "        # 找到测试集对应训练集中的元组，因为可能出现元组只有一部分出现在训练集中\n",
    "        def matchvocab(gram):\n",
    "            masks = self.masks if self.laplace else list(reversed(list(product((0,1), repeat=n_of_KN))))\n",
    "            gram = (gram, ) if type(gram) is str else gram # 为了匹配之前那种unigram的key也是元组的情况\n",
    "            for mask in masks:\n",
    "                possible_in  = tuple((token if sign == 1 else '<UNK>' for token, sign in zip(gram, mask)))\n",
    "                if n_of_KN:\n",
    "                    if possible_in in self.model[self.n-n_of_KN]:\n",
    "                        return possible_in\n",
    "                else:\n",
    "                    if possible_in in self.model:\n",
    "                        return possible_in\n",
    "        transform_gram = (matchvocab(gram) for gram in test_gram)\n",
    "        if n_of_KN:\n",
    "            probabilities = [self.model[self.n-n_of_KN][gram] for gram in transform_gram]\n",
    "        else:\n",
    "            probabilities = [self.model[gram] for gram in transform_gram]\n",
    "        return math.exp((-1/N)*sum(map(math.log, probabilities)))\n",
    "    \n",
    "    def choose_next(self, prev, drop, i=0, n_of_KN=None):\n",
    "        '''\n",
    "        给定前面的词，生成下一个词，由于我的模型采取的是取概率最大的词，因此要\n",
    "        输入一个drop list，以免生成的词前面出现过，否则就会一直重复。\n",
    "        '''\n",
    "        drop = ['<UNK>']+drop\n",
    "        # 先考虑KN平滑模型的情况\n",
    "        if n_of_KN:\n",
    "            candidate = ((ngram[-1], prob) for ngram, prob in self.model[self.n-n_of_KN].items() if ngram[:-1]==prev)\n",
    "            candidate = filter(lambda candidate:candidate[0] not in drop, candidate)\n",
    "            sorted_vocab = sorted(candidate, key=lambda cand:cand[1], reverse=True)\n",
    "#             print(len(sorted_vocab))\n",
    "#             print(sorted_vocab)\n",
    "            if len(sorted_vocab):\n",
    "                try:\n",
    "                    return sorted_vocab[i] if sorted_vocab[i][0] !='</s>' else sorted_vocab[i+1]\n",
    "                except:\n",
    "                    return ('</s>',1)\n",
    "            else:\n",
    "                return ('</s>',1)\n",
    "        # laplace模型\n",
    "        #依旧是先处理n=1的情况\n",
    "        if self.n == 1:\n",
    "            sorted_vocab = sorted(self.model.items(), key= lambda token:token[1], reverse=True)\n",
    "            filtered_vocab = list(filter(lambda candidate:candidate[0][0] not in drop, sorted_vocab))\n",
    "            return (filtered_vocab[0][0][0],filtered_vocab[0][1]) if len(filtered_vocab) else ('</s>',1)\n",
    "        # 处理n>1的情况\n",
    "        candidate = ((ngram[-1], prob) for ngram, prob in self.model.items() if ngram[:-1]==prev)\n",
    "        candidate = filter(lambda candidate:candidate[0] not in drop, candidate)\n",
    "        sorted_vocab = sorted(candidate, key=lambda cand:cand[1], reverse=True)\n",
    "        if len(sorted_vocab):\n",
    "            return sorted_vocab[i] if sorted_vocab[i][0] !='</s>' else sorted_vocab[i+1]\n",
    "        else:\n",
    "            return ('</s>',1)\n",
    "    \n",
    "    def generate_sentence(self, num_sent, sent_start, max_len = 12, n_of_KN = None):\n",
    "        '''\n",
    "        Input:\n",
    "        num_sent：要生成的句子数量\n",
    "        sent_start：给定的句子开头（一个单词）\n",
    "        max_len：要生成的句子的最大长度\n",
    "        '''\n",
    "        what_n = self.n if self.laplace else n_of_KN\n",
    "        for i in range(num_sent):\n",
    "            sent, prob = ['<s>']*max(1,what_n-1)+[sent_start[i]], 1\n",
    "            while sent[-1] != '</s>':\n",
    "                prev = tuple(sent[-(what_n-1):]) if what_n != 1 else ()\n",
    "                if n_of_KN: # KN模型\n",
    "                    next_token, new_prob = self.choose_next(prev, sent, i, n_of_KN)\n",
    "                else:  # laplace模型\n",
    "                    next_token, new_prob = self.choose_next(prev, sent, i)\n",
    "                sent.append(next_token)\n",
    "                prob *= new_prob\n",
    "                if len(sent) >= max_len:\n",
    "                    sent.append('</s>')\n",
    "            print('The',i,'th sentence:\\n',' '.join(sent),'\\nProbability is',prob)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Smoothing\n",
    "### Unigram\n",
    "先处理Unigram模型，训练模型，并在测试集上计算测试集困惑度。  \n",
    "给定五个单词（也可以给定多个单词或者其他数量的开头组别）开头，生成五条句子（**句子的最大长度默认设置为12**，后面保持一致）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of Unigram using Laplace Smoothing is 1183.6832139398616\n"
     ]
    }
   ],
   "source": [
    "pretrain_3.extend(['<UNK>','<UNK>','<UNK>'])\n",
    "laplace_1 = LM(pretrain_3, 1, laplace=1)\n",
    "print('The perplexity of Unigram using Laplace Smoothing is',laplace_1.perplexity(pretest_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genrate sentence given ['he', 'like', 'the', 'rainy', 'day']\n",
      "The 0 th sentence:\n",
      " <s> he the </s> \n",
      "Probability is 0.0023393517362952317\n",
      "The 1 th sentence:\n",
      " <s> like the </s> \n",
      "Probability is 0.0023393517362952317\n",
      "The 2 th sentence:\n",
      " <s> the </s> \n",
      "Probability is 0.03994934901695986\n",
      "The 3 th sentence:\n",
      " <s> rainy the </s> \n",
      "Probability is 0.0023393517362952317\n",
      "The 4 th sentence:\n",
      " <s> day the </s> \n",
      "Probability is 0.0023393517362952317\n"
     ]
    }
   ],
   "source": [
    "start = ['he','like','the','rainy','day']\n",
    "print('Genrate sentence given',start)\n",
    "laplace_1.generate_sentence(5,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上面的句子根本不成句子，还有一个重要的特点，每条句子都是由单词*the*结尾。这很好理解,Unigram不考虑词和词之间的关联性，统计的词频字典的排列完全是依赖于单词的出现概率。而我的模型的生成句子的机制为选择**句子之前没有出现过的**，**概率最高**的词作为下一个单词。众所周知，如果不对数据进行一些删除停用词之类的处理，那么出现频率最高的词大多都是一些无异议的助词、副词等，单词*the*就符合这样的条件。因此，我们可以推测，*the*这个单词的出现频率相当之高，END标识符的频率则仅次*the*。  \n",
    "接下来，让我们查看模型的词典验证猜想："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('<s>',), 0.07989869803391972),\n",
       " (('the',), 0.05855794384289209),\n",
       " (('</s>',), 0.03994934901695986),\n",
       " (('of',), 0.028814567777359162),\n",
       " (('and',), 0.025821999133618743)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看一下模型的词典排序前五的单词\n",
    "sorted(laplace_1.model.items(), key= lambda token:token[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，词频排序第一的是start标识符，由于每个句子开头都会有该标识符，因此生成句子时不会重复考虑该词。而*the*果然排在仅次于start标识符的第二位，且end标识符也在*the*后面，上面的猜想得到验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除模型，节省内存\n",
    "del laplace_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of Bigrams using Laplace Smoothing is 1198.6933669904665\n"
     ]
    }
   ],
   "source": [
    "laplace_2 = LM(pretrain_3, 2, laplace = 1)\n",
    "print('The perplexity of Bigrams using Laplace Smoothing is',laplace_2.perplexity(pretest_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来生成句子，收到之前Unigram的启发，我将模型的生成句子功能稍作修改，每次不是选择概率最大的词组，而是给定了一个可选参数i（选择概率第i大的词组）,可以调节这个i使得多句话之间尽量避开出现重复的可能性，也增加了生成句子结果的丰富性，当然，这个i会比较小，能够确保生成的句子依旧是联合概率接近最大的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genrate sentence given ['wrote', 'picture', 'feature', 'he', 'a']\n",
      "The 0 th sentence:\n",
      " <s> wrote that the film s first time yūko and a new </s> \n",
      "Probability is 4.0851438666176433e-23\n",
      "The 1 th sentence:\n",
      " <s> picture of his father was the first film s work on </s> \n",
      "Probability is 1.450105434062245e-19\n",
      "The 2 th sentence:\n",
      " <s> feature of a few years in which is an average grade </s> \n",
      "Probability is 1.0688741125943289e-21\n",
      "The 3 th sentence:\n",
      " <s> he also been described it s work in which had not </s> \n",
      "Probability is 2.979635861755352e-21\n",
      "The 4 th sentence:\n",
      " <s> a result was an attempt on november 2017 in its second </s> \n",
      "Probability is 1.4837724352278043e-25\n"
     ]
    }
   ],
   "source": [
    "# 生成句子\n",
    "start = ['wrote','picture','feature','he','a']\n",
    "print('Genrate sentence given',start)\n",
    "laplace_2.generate_sentence(5,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到句子生成正常了许多，但其内在还是比较无逻辑。  \n",
    "和Unigram对比，我们也来看看排序比较前的unigram组合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('</s>', '<s>'), 0.9047834143512261),\n",
       " (('<s>', '<s>'), 0.4750061865555504),\n",
       " (('of', 'the'), 0.24517574550985746),\n",
       " (('at', 'the'), 0.2058128664004922),\n",
       " (('in', 'the'), 0.1950417376517553),\n",
       " (('for', 'the'), 0.1561703800318033),\n",
       " (('on', 'the'), 0.1487876333528144),\n",
       " (('from', 'the'), 0.1375576973545409),\n",
       " (('as', 'a'), 0.1345384198990141),\n",
       " (('with', 'the'), 0.12908515638403562)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看一下模型的词典排序前十的单词\n",
    "sorted(laplace_2.model.items(), key= lambda token:token[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到出现频率比较高的还是一些无意义的副词，不过好在我们的生成句子不会生成重复的词，也不会生成start和end标识符，因此扩充了生成句子的多样性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of Trigrams using Laplace Smoothing is 5064.635667231597\n"
     ]
    }
   ],
   "source": [
    "laplace_3 = LM(pretrain_3, 3, laplace = 1)\n",
    "print('The perplexity of Trigrams using Laplace Smoothing is',laplace_3.perplexity(pretest_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genrate sentence given ['feature', 'wrote']\n",
      "The 0 th sentence:\n",
      " <s> <s> feature animation and the united states on november 6 2012 </s> \n",
      "Probability is 1.9973856131544054e-28\n",
      "The 1 th sentence:\n",
      " <s> <s> wrote in his book on a new constitution was adopted </s> \n",
      "Probability is 8.587839036450261e-32\n",
      "Genrate sentence given ['picture', 'he', 'a']\n",
      "The 0 th sentence:\n",
      " <s> <s> picture of the film s release date was pushed back </s> \n",
      "Probability is 1.1350961028740091e-26\n",
      "The 1 th sentence:\n",
      " <s> <s> he also said the book s title and was released </s> \n",
      "Probability is 3.2580665152331657e-28\n",
      "The 2 th sentence:\n",
      " <s> <s> a new government and that it would not seek to </s> \n",
      "Probability is 2.5961921770238573e-27\n"
     ]
    }
   ],
   "source": [
    "# 生成句子\n",
    "start = ['feature','wrote']\n",
    "print('Genrate sentence given',start)\n",
    "laplace_3.generate_sentence(2,start)\n",
    "start = ['picture','he','a']\n",
    "print('Genrate sentence given',start)\n",
    "laplace_3.generate_sentence(3,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到生成的句子变得通顺许多！  \n",
    "照例，查看一下模型词典概率较高的词组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('</s>', '<s>', '<s>'), 0.9047838590651693),\n",
       " (('<s>', '<s>', 'the'), 0.11044467272539604),\n",
       " (('the', 'united', 'states'), 0.09821698398307646),\n",
       " (('one', 'of', 'the'), 0.08719148356118002),\n",
       " (('<s>', '<s>', 'in'), 0.08073658289629546),\n",
       " (('as', 'well', 'as'), 0.06930922358545853),\n",
       " (('film', '</s>', '<s>'), 0.06653110197709214),\n",
       " (('<UNK>', '</s>', '<s>'), 0.06356196433491727),\n",
       " (('<s>', 'he', 'was'), 0.06354765257219022),\n",
       " (('<s>', 'in', 'the'), 0.06316536352487981)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看一下模型的词典排序前十的单词\n",
    "sorted(laplace_3.model.items(), key= lambda token:token[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到会出现比较多的start标识符和end标识符，这是正常的，因为输入的句子和句子连接，导致end标识符后面几乎一定是start标识符。  \n",
    "最后，来总结一下困惑度：\n",
    "+ The perplexity of Unigram using Laplace Smoothing is 1183.6832139398616\n",
    "+ The perplexity of Bigrams using Laplace Smoothing is 1198.6933669904665\n",
    "+ The perplexity of Trigrams using Laplace Smoothing is 5064.635667231597  \n",
    "\n",
    "可以看到困惑度在trigram的模型下反而有一个异常的上升，我对此查阅资料，提出了以下几个猜想：\n",
    "1. 困惑度在样本量比较小的情况下波动会比较大，并不是严格下降的。我查阅到其他人做困惑度和LDA主题数关系的时候发现，困惑度的波动值非常大，但总体的趋势是下降的。\n",
    "2. 困惑度越小代表这个句子的概率越大。因此也有可能是因为使用的数据集还不够大，并未统计出较为完善的词库，导致训练集和测试集之间的句子风格差异还是比较大，因此预测比较差。\n",
    "3. 在数据处理时，我将一部分只出现了一次处理为了UNK，这导致一些原本不一样的词组被同类化，因此其条件概率就降低了，句子的联合概率降低，导致困惑度上升。\n",
    "无论是哪一种原因导致，都需后续进行优化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KN Smoothing\n",
    "与laplace平滑不同的是，由于KN平滑在计算中是迭代计算的。为了优化我的模型的计算效率，我在计算时保留所有用于迭代的概率。也就是说，只要计算了trigram的KN平滑，就能同时得到unigram和bigram的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型，直接训trigram\n",
    "KN_3 = LM(pretrain_3,3,KN=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of Unigrams using KN Smoothing is 1189.0227997987856\n",
      "The perplexity of Bigrams using KN Smoothing is 141.54655339809918\n",
      "The perplexity of Trigrams using KN Smoothing is 31.628652819181333\n"
     ]
    }
   ],
   "source": [
    "# 计算困惑度\n",
    "print('The perplexity of Unigrams using KN Smoothing is',KN_3.perplexity(pretest_3,1))\n",
    "print('The perplexity of Bigrams using KN Smoothing is',KN_3.perplexity(pretest_3,2))\n",
    "print('The perplexity of Trigrams using KN Smoothing is',KN_3.perplexity(pretest_3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到这个困惑度相当标准，从unigram到bigram再到trigram呈递减的形式，这是因为n越大，KN平滑的模型效果就越好，因此在真实数据（或者说正确数据）上的概率越大，因此困惑度也越小。  \n",
    "另一方面，会发现KN平滑整体的困惑度比laplace平滑小的多，这说明KN平滑是比laplace平滑更加好的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genrate sentence given ['feature', 'wrote']\n",
      "Unigram model:\n",
      "The 0 th sentence:\n",
      " <s> feature the of and in to a was s that he </s> \n",
      "Probability is 3.08060513522822e-18\n",
      "The 1 th sentence:\n",
      " <s> wrote of and in to a was s that he as </s> \n",
      "Probability is 4.220595325284688e-19\n",
      "Bigram model:\n",
      "The 0 th sentence:\n",
      " <s> feature film s first time in the united states and a </s> \n",
      "Probability is 3.230001888633974e-12\n",
      "The 1 th sentence:\n",
      " <s> wrote the first film was not be used in his father </s> \n",
      "Probability is 7.534654687096079e-15\n",
      "Trigram model:\n",
      "The 0 th sentence:\n",
      " <s> <s> feature animation and the united states on november 6 2012 </s> \n",
      "Probability is 1.9005014884189034e-09\n",
      "The 1 th sentence:\n",
      " <s> <s> wrote in his book on a new constitution was adopted </s> \n",
      "Probability is 1.0987431096048057e-12\n",
      "Genrate sentence given ['picture', 'he', 'a']\n",
      "Unigram model:\n",
      "The 0 th sentence:\n",
      " <s> picture the of and in to a was s that he </s> \n",
      "Probability is 3.08060513522822e-18\n",
      "The 1 th sentence:\n",
      " <s> he of and in to a was s that as for </s> \n",
      "Probability is 4.142636971375308e-19\n",
      "The 2 th sentence:\n",
      " <s> a of and in to was s that he as for </s> \n",
      "Probability is 1.7699682592020075e-19\n",
      "Bigram model:\n",
      "The 0 th sentence:\n",
      " <s> picture of the film s first time in a new york </s> \n",
      "Probability is 1.099135743562444e-12\n",
      "The 1 th sentence:\n",
      " <s> he had a member and his father was not be used </s> \n",
      "Probability is 4.837977059539752e-16\n",
      "The 2 th sentence:\n",
      " <s> a few years later that it s work of her husband </s> \n",
      "Probability is 5.150606136851034e-16\n",
      "Trigram model:\n",
      "The 0 th sentence:\n",
      " <s> <s> picture of the film s release date was pushed back </s> \n",
      "Probability is 3.5023327167272474e-08\n",
      "The 1 th sentence:\n",
      " <s> <s> he also served on a new constitution was adopted as </s> \n",
      "Probability is 5.135559129925151e-12\n",
      "The 2 th sentence:\n",
      " <s> <s> a new government and that it would not seek the </s> \n",
      "Probability is 8.401232804133572e-13\n"
     ]
    }
   ],
   "source": [
    "# 每个模型生成5个句子\n",
    "start = ['feature','wrote']\n",
    "print('Genrate sentence given',start)\n",
    "print('Unigram model:')\n",
    "KN_3.generate_sentence(2,start,n_of_KN=1)\n",
    "print('Bigram model:')\n",
    "KN_3.generate_sentence(2,start,n_of_KN=2)\n",
    "print('Trigram model:')\n",
    "KN_3.generate_sentence(2,start,n_of_KN=3)\n",
    "start = ['picture','he','a']\n",
    "print('Genrate sentence given',start)\n",
    "print('Unigram model:')\n",
    "KN_3.generate_sentence(3,start,n_of_KN=1)\n",
    "print('Bigram model:')\n",
    "KN_3.generate_sentence(3,start,n_of_KN=2)\n",
    "print('Trigram model:')\n",
    "KN_3.generate_sentence(3,start,n_of_KN=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到生成的句子依旧符合之前的规律,gram的滑动窗口越大，生成的句子越有逻辑。其中Unigram不再像之前那样只生成一个或两个单词，是因为前面对模型的生成句子功能进行了一些修改，使其如果选到end标识符则跳过选择下一个频率最大的候选词。因此它的输出长度变长了一些。  \n",
    "照例，我们看一下模型训练出来的词典概率较大的词组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model:\n",
      "[(('<s>',), 0.07989219120579981), (('the',), 0.05856060737906329), (('</s>',), 0.039946095602899905), (('of',), 0.02882285065211335), (('and',), 0.025824499397013943), (('in',), 0.023258439689003544), (('to',), 0.022127029785779476), (('a',), 0.018937303642181602), (('was',), 0.010778774491548017), (('s',), 0.009660883871183065)]\n",
      "Bigram Model:\n",
      "[(('</s>', '<s>'), 0.9999999500160902), (('doesn', 't'), 0.9999614251294366), (('wasn', 't'), 0.9999263467283496), (('isn', 't'), 0.9999171934634385), (('couldn', 't'), 0.9998989408598283), (('wouldn', 't'), 0.9998744378270983), (('didn', 't'), 0.9996773555355648), (('hadn', 't'), 0.9996723033126893), (('weren', 't'), 0.9996611949504075), (('shouldn', 't'), 0.9996600425522797)]\n",
      "Trigram Model:\n",
      "[(('film', '</s>', '<s>'), 0.9999999999996378), (('<UNK>', '</s>', '<s>'), 0.9999999999996199), (('him', '</s>', '<s>'), 0.999999999999544), (('it', '</s>', '<s>'), 0.9999999999995405), (('life', '</s>', '<s>'), 0.9999999999994994), (('time', '</s>', '<s>'), 0.9999999999993631), (('election', '</s>', '<s>'), 0.9999999999992292), (('years', '</s>', '<s>'), 0.9999999999992268), (('them', '</s>', '<s>'), 0.9999999999992054), (('war', '</s>', '<s>'), 0.9999999999992006)]\n"
     ]
    }
   ],
   "source": [
    "# 查看一下模型的词典排序前十的单词\n",
    "print('Unigram Model:')\n",
    "print(sorted(KN_3.model[2].items(), key= lambda token:token[1], reverse=True)[:10])\n",
    "print('Bigram Model:')\n",
    "print(sorted(KN_3.model[1].items(), key= lambda token:token[1], reverse=True)[:10])\n",
    "print('Trigram Model:')\n",
    "print(sorted(KN_3.model[0].items(), key= lambda token:token[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram排名较前的都是一些副词，是合理的。  \n",
    "可以看到Bigram排名较前的多是一些否定副词的前后缀，这主要是由于我们在处理数据时，要求去除所有除字母和数字的特殊符号，导致否定形式的前后被拆分为两部分，这其实是不太准确的，后续改进可以从数据处理角度再改进。  \n",
    "Trigram就更奇特了，可以看到前面的几个组合概率大多接近1，且最后两个词均为（end标识符，start标识符）。仔细一想，这也很容易理解，因为trigram要三个词组成，而我们输入的数据都是句子接句子，那么end标识符的下一个词一定是start标识符，无论第一个词是什么。不过这个现象并不会影响我们的句子生成和困惑度计算，反而，要有这个性质，我们的困惑度计算才是准确的（因为不希望句子和句子之间的衔接影响概率计算）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二题到此为止，删除训练好的模型，节省内存\n",
    "del laplace_2\n",
    "del laplace_3\n",
    "del KN_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "构建朴素贝叶斯分类器，对文本进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建NB模型\n",
    "class NaiveBayes():\n",
    "    def __init__(self, class_counter, train_data, laplace = 1):\n",
    "        '''\n",
    "        Input:\n",
    "        class_counter:每个类别的文档数\n",
    "        train_data:训练的文档数据 一个大字典，key为类别，value为列表，里面是token\n",
    "        train_labels:训练数据对应的类别\n",
    "        laplace:laplace平滑的参数\n",
    "        '''\n",
    "        self.class_counter = class_counter\n",
    "        self.vocab = set([y for x in train_data.values() for y in x ])\n",
    "        self.vocabsize = len(self.vocab)+1 # 记得加上UNK\n",
    "        self.token_class_dict = defaultdict(dict)\n",
    "        self.num_token_class = defaultdict(int)\n",
    "        self.data = train_data\n",
    "        self.laplace = laplace\n",
    "        \n",
    "    def create_model(self):\n",
    "        '''\n",
    "        训练NB模型\n",
    "        '''\n",
    "        for label, token in self.data.items():\n",
    "            self.token_class_dict[label] = nltk.FreqDist(token)\n",
    "            self.num_token_class[label] = len(token)\n",
    "            \n",
    "    def predict_text(self, text):\n",
    "        '''\n",
    "        给定一个文本text，预测其类别\n",
    "        Input:\n",
    "        text:处理过后的text，为一个大列表,里面是token\n",
    "        '''\n",
    "        predict_prob = {}\n",
    "        for label, num_text in self.class_counter.items():\n",
    "            predict_prob[label] = num_text  # 这里不除以文档数是因为对比较结果无影响，且也可以预防概率过小不精确\n",
    "            for token in text:\n",
    "                temp = self.token_class_dict[label][token] if token in self.token_class_dict[label] else 0\n",
    "                predict_prob[label] *= (temp + self.laplace)/(self.num_token_class[label]+self.laplace*self.vocabsize)\n",
    "        pred_label = max(predict_prob.items(), key=lambda x:x[1])[0]\n",
    "        return pred_label\n",
    "    \n",
    "    def predict_test(self, testdata, tokens):\n",
    "        '''\n",
    "        在测试集上预测\n",
    "        Input:\n",
    "        testdata:处理后的测试数据，为一个大列表，里面嵌套小列表，每个小列表为一个text\n",
    "        为了防止有些文本的过长，对每个文档我只取最多10000个tokens\n",
    "        '''\n",
    "        return [self.predict_text(text[:tokens]) for text in testdata]\n",
    "    \n",
    "    def report(self, testdata, test_labels, tokens = 10000):\n",
    "        '''\n",
    "        计算并报告模型在测试集上的准确率和Micro-F1 分数和 Macro-F1 分数\n",
    "        '''\n",
    "        pred_labels = self.predict_test(testdata, tokens)\n",
    "        ac = sum([1 if pred_labels[i]==test_labels[i] else 0 for i in range(len(test_labels))])/len(test_labels)\n",
    "        print('The accuracy of the test data is',ac)\n",
    "        print('The Micro-F1 of the test data is',f1_score(test_labels,pred_labels,average='micro'))\n",
    "        print('The Macro-F1 of the test data is',f1_score(test_labels,pred_labels,average='macro'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'processed_data1'\n",
    "f1 = open(path, 'rb')\n",
    "data = pickle.load(f1)\n",
    "path = 'processed_data2'\n",
    "f1 = open(path, 'rb')\n",
    "data.extend(pickle.load(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 490952.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# 因为需要使用类别数据和原始对应的标签，因此这里再统计一次数据\n",
    "class_counter = defaultdict(int)\n",
    "data_labels = []\n",
    "for record in tqdm(wiki_data):\n",
    "    class_counter[record['label']] += 1\n",
    "    data_labels.append(record['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分带标签的数据集\n",
    "data_with_label = [[data[i], data_labels[i]] for i in range(len(data))]\n",
    "traindata_with_label, testdata_with_label = split_data(data_with_label,1,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将数据处理成需要的输入格式\n",
    "train_dict_30 = defaultdict(list)\n",
    "class_counter_30 = defaultdict(int)\n",
    "train_dict_50 = defaultdict(list)\n",
    "class_counter_50 = defaultdict(int)\n",
    "train_dict_70 = defaultdict(list)\n",
    "class_counter_70 = defaultdict(int)\n",
    "train_dict_90 = defaultdict(list)\n",
    "class_counter_90 = defaultdict(int)\n",
    "for i in tqdm(range(9000)):\n",
    "    if i < 9000*0.3:\n",
    "        text, text_label = traindata_with_label[i]\n",
    "        flatten_text = [y for x in text for y in x]\n",
    "        train_dict_30[text_label].extend(flatten_text)\n",
    "        train_dict_50[text_label].extend(flatten_text)\n",
    "        train_dict_70[text_label].extend(flatten_text)\n",
    "        train_dict_90[text_label].extend(flatten_text)\n",
    "        class_counter_30[text_label] += 1\n",
    "        class_counter_50[text_label] += 1\n",
    "        class_counter_70[text_label] += 1\n",
    "        class_counter_90[text_label] += 1\n",
    "    elif i < 9000*0.5:\n",
    "        text, text_label = traindata_with_label[i]\n",
    "        flatten_text = [y for x in text for y in x]\n",
    "        train_dict_50[text_label].extend(flatten_text)\n",
    "        train_dict_70[text_label].extend(flatten_text)\n",
    "        train_dict_90[text_label].extend(flatten_text)\n",
    "        class_counter_50[text_label] += 1\n",
    "        class_counter_70[text_label] += 1\n",
    "        class_counter_90[text_label] += 1\n",
    "    elif i < 9000*0.7:\n",
    "        text, text_label = traindata_with_label[i]\n",
    "        flatten_text = [y for x in text for y in x]\n",
    "        train_dict_70[text_label].extend(flatten_text)\n",
    "        train_dict_90[text_label].extend(flatten_text)\n",
    "        class_counter_70[text_label] += 1\n",
    "        class_counter_90[text_label] += 1\n",
    "    else:\n",
    "        text, text_label = traindata_with_label[i]\n",
    "        flatten_text = [y for x in text for y in x]\n",
    "        train_dict_90[text_label].extend(flatten_text)\n",
    "        class_counter_90[text_label] += 1\n",
    "# 处理测试数据\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for i in tqdm(testdata_with_label):\n",
    "    test_texts.append([y for x in i[0] for y in x])\n",
    "    test_labels.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别构建不同训练集的模型，并进行训练\n",
    "NB_30 = NaiveBayes(class_counter_30, train_dict_30)\n",
    "NB_30.create_model()\n",
    "NB_50 = NaiveBayes(class_counter, train_dict_50)\n",
    "NB_50.create_model()\n",
    "NB_70 = NaiveBayes(class_counter, train_dict_70)\n",
    "NB_70.create_model()\n",
    "NB_90 = NaiveBayes(class_counter, train_dict_90)\n",
    "NB_90.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model using 30% train data:\n",
      "The accuracy of the test data is 0.887\n",
      "The Micro-F1 of the test data is 0.887\n",
      "The Macro-F1 of the test data is 0.6805092966288969\n",
      "For model using 50% train data:\n",
      "The accuracy of the test data is 0.885\n",
      "The Micro-F1 of the test data is 0.885\n",
      "The Macro-F1 of the test data is 0.6795896767380041\n",
      "For model using 70% train data:\n",
      "The accuracy of the test data is 0.893\n",
      "The Micro-F1 of the test data is 0.893\n",
      "The Macro-F1 of the test data is 0.6874751559150226\n",
      "For model using 90% train data:\n",
      "The accuracy of the test data is 0.895\n",
      "The Micro-F1 of the test data is 0.895\n",
      "The Macro-F1 of the test data is 0.7687731308554194\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "print('For model using 30% train data:')\n",
    "NB_30.report(test_texts,test_labels,80)\n",
    "print('For model using 50% train data:')\n",
    "NB_50.report(test_texts,test_labels,80)\n",
    "print('For model using 70% train data:')\n",
    "NB_70.report(test_texts,test_labels,80)\n",
    "print('For model using 90% train data:')\n",
    "NB_90.report(test_texts,test_labels,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察模型的测试结果，可以看到基本上是符合数据集越大，Micro-F1和Macro-F1的数值趋势是越来越大的。这也是符合逻辑的，因为数据集越大，我的模型就越符合真实的概率分布模型，因此对测试数据的分类也越准确。  \n",
    "第二个观察到的点是，Macro-F1总是比Micro-F1小一些。这是由于从定义上来说，Macro-F1平等地看待各个类别，它的值会受到稀有类别的影响，也就说它更容易被一些难以预测的类别拉低正确率。  \n",
    "第三个点是，最后的预测中，我采取的测试集输入数据是只取了每个文本的前80个token。这个是试出来的。在此之前，我所有的模型的准确率都为0.22，这个数字和完全无训练的分类模型一模一样！这是由于NB模型是一个概率相乘的模型，词数越多，相乘的小于1的数字就越多，最终的句子概率呈现**指数下降**的趋势，而又由于计算机的存储精度并不高，最终导致所有句子的预测概率存储都变为了0，分类也是完全随机的了。因此，适当地减少一些输入测试数据是合理的。经过测试（**在验证集上**），当token取0-85时，模型的效果随着数据量的增加而增加，当token取85-100时，模型的效果随着数据量的增加而锐减，大于100后，效果几近于无。因此，我最终选取的token数为80.  \n",
    "**ADD NOTE：**  \n",
    "后面我又想到，解决这个问题其实可以使用log变换，不仅可以放大概率的数值，乘法变成加法也不会再出现概率消减的情况。后来尝试了这种方法后，发现最终模型效果并没有进一步提升，因此这里不再改动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
