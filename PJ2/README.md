| **ask 1.** Train word embeddings using SGNS: Use our enwiki_20220201.json as training data. You can use the [Gensim tool](https://radimrehurek.com/gensim/models/word2vec.html). |
| ------------------------------------------------------------ |
| **Task 2.** Find similar/dissimilar word pairs.              |
| **Task 3.** Present a document as an embedding: For each document, you have several choices to generate document embedding:Use the average of embeddings of all words in each document.Use the first paragraphâ€™s words and take an average on these embeddings.Use the doc2vec algorithm to present each document.Use the k-means clustering algorithm to cluster these document embeddings. Since you have the ground truth cluster size (k=10), you can use the Micro/Macro F1-score to measure the performance of these choices. |
| **Task 4.** Use t-SNE to project these vectors into 2-d and plot them out for each of the above choices. Each point should have a specific color (represent a particular cluster). You may need to try different parameters of t-SNE. Can find more details about t-SNE in this excellent article ([t-SNE](https://distill.pub/2016/misread-tsne/)). |
| **Bonus option 1:** If Task 1 is implemented by hand correctly, you will be rewarded 15 bonus points. You can use the Skip-Gram with the negative sampling model, the CBOW model, or any model you are favor. Just explain your model clearly. |
| **Bonus option 2:** Any new method that can significantly improve the performance other than these 3 options in Task 3 will be rewarded with 15 bonus points. The novelty may come from a better document embedding or a better clustering algorithm. (Hint: From the word embedding perspective, finding the most relevant word embeddings that represent these documents is the key.) |